{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.18it/s]\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.31it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.66it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.57it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.21it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.31it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.30it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  8.08it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.66it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.46it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.00it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.51it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.63it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.63it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.60it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.74it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.45it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.21it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.42it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.37it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.45it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.62it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.42it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 27.71it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.41it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.72it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.03it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.06it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.79it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.08it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.62it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 27.56it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.48it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.70it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.36it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.64it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.49it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.45it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.42it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.82it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.59it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.37it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.50it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.25it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.51it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.23it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.20it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.47it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.49it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.20it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.77it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.68it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.57it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.65it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.77it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.57it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.87it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.50it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.27it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.70it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.80it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.59it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.85it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.71it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.00it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.37it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.70it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.47it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.51it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.53it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.18it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.65it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.99it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.64it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.38it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.65it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.28it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.64it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.29it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.58it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.34it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.69it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.59it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.79it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 24.97it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.28it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.93it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.61it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.20it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.60it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.01it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.62it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.74it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.75it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 24.37it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.24it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.61it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.23it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.51it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.59it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.67it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.58it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.61it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.77it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.58it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.72it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.77it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.57it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.75it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.08it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.67it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.09it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.65it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 27.37it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  6.93it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.45it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.67it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.78it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.61it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.81it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.69it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.80it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.64it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.65it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.64it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.24it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.51it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.16it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.82it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.49it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.75it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.89it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.58it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.70it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.53it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.43it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.76it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.64it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.56it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.96it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.79it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.67it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.53it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.58it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.79it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.97it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.75it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.24it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.56it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.96it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.60it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.54it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.70it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.81it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.71it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.70it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.73it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.74it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.50it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.45it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.68it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Sin mejora (7/10) - Mejor score: 0.0996\n",
      "[Fold 3, Paso 010] Loss: -0.0034, Reward: 0.0169 | Train -> aest: -0.0018, align: 0.2362, pick: 20.0943, hps: 24.9383, weighted: 0.1320 | Val -> aest: -0.0042, align: 0.2607, pick: 19.5263, hps: 25.3752, weighted: 0.1002\n",
      "✅ Guardados 264 pesos LoRA en cv_results\\fold_02\\lora_weights_fold_02.pth\n",
      "✅ Fold 3: entrenamiento completado. Pesos guardados en cv_results\\fold_02\\lora_weights_fold_02.pth\n",
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.42it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.26it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 26.87it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.32it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.37it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.64it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.66it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.67it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.17it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.49it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.84it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.56it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.33it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.67it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.32it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.50it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.37it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.53it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.89it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.63it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.51it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.38it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.68it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.44it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.47it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.48it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.07it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.51it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.54it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.48it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.22it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.61it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 31.20it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.72it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.00it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.55it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 29.47it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.49it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.68it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.69it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.20it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.65it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.51it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.51it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 27.91it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.60it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.13it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.30it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 28.33it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.68it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.72it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.70it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.22it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.31it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnCLIP] steps -> prior=25, decoder=25 (truncate_last_k=25), super_res=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.58it/s]\n",
      "100%|██████████| 25/25 [00:03<00:00,  7.63it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import inspect\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TVF\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "import open_clip\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RLConfig:\n",
    "    model_path: str = r\"C:\\Users\\MiguelDiaz - GenImag\\Documents\\proyecto IA\\Generador\"\n",
    "    cache_dir: str = r\"C:\\IA RL\\IA\\cache\"\n",
    "    train_prompts_file: str = \"training_prompts_expanded.txt\"\n",
    "    val_prompts_file: str = \"validation_prompts_400.txt\"\n",
    "    batch_size: int = 20\n",
    "    lr: float = 1e-5\n",
    "    steps: int = 50\n",
    "    grad_accum_steps: int = 5\n",
    "    max_grad_norm: float = 1.0\n",
    "    prior_steps: int = 25\n",
    "    decoder_steps: int = 25\n",
    "    sr_steps: int = 10\n",
    "    prior_guidance: float = 4.0\n",
    "    decoder_guidance: float = 8\n",
    "    w_aesthetic: float = 0.4\n",
    "    w_clip_align: float = 0.4\n",
    "    w_pickscore: float = 0.15\n",
    "    w_hpsv2: float = 0.05\n",
    "    reward_norm: str = \"none\"\n",
    "    eval_every: int = 5\n",
    "    eval_dir: str = \"eval_samples\"\n",
    "    save_n_grid: int = 4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    k_folds: int = 4\n",
    "    test_ratio: float = 0.1\n",
    "    eval_batch_size: int = 2\n",
    "    pickscore_model_path: Optional[str] = r\"C:\\Users\\MiguelDiaz - GenImag\\Documents\\proyecto IA\\Evaluadores\\pickscore\"\n",
    "    hpsv2_model_path: Optional[str] = r\"C:\\Users\\MiguelDiaz - GenImag\\Documents\\proyecto IA\\Evaluadores\\hpsv2\\HPSv2-hf\"\n",
    "    seed: int = 42\n",
    "    early_stopping_patience: int = 10\n",
    "    min_delta: float = 0.001\n",
    "    lora_rank: int = 4\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.1\n",
    "    param_search_n: int = 0\n",
    "    lora_rank_options: Tuple[int, ...] = (20, 4, 8, 16)\n",
    "    lora_alpha_options: Tuple[int, ...] = (8, 16, 32, 64)\n",
    "    lora_dropout_options: Tuple[float, ...] = (0.05, 0.1, 0.2)\n",
    "    lr_options: Tuple[float, ...] = (1e-5, 5e-5, 1e-4)\n",
    "    w_aesthetic_options: Tuple[float, ...] = (0.1, 0.2, 0.4, 0.6)\n",
    "    w_clip_align_options: Tuple[float, ...] = (0.1, 0.2, 0.4, 0.6)\n",
    "    w_pickscore_options: Tuple[float, ...] = (0.0, 0.1, 0.15, 0.07)\n",
    "    w_hpsv2_options: Tuple[float, ...] = (0.0, 0.01, 0.02, 00.3)\n",
    "    train_decoder_steps: Optional[int] = None\n",
    "    eval_decoder_steps: Optional[int] = None\n",
    "\n",
    "    @property\n",
    "    def num_inference_steps(self) -> int:\n",
    "        return self.decoder_steps\n",
    "\n",
    "\n",
    "def load_unclip_pipeline(cfg: RLConfig) -> DiffusionPipeline:\n",
    "    is_windows_path = (\n",
    "        cfg.model_path.startswith(('C:', 'D:', 'E:', 'F:', '\\\\', '/')) or\n",
    "        ':\\\\' in cfg.model_path or\n",
    "        cfg.model_path.startswith('.')\n",
    "    )\n",
    "    if is_windows_path:\n",
    "        model_path_normalized = os.path.normpath(cfg.model_path)\n",
    "        model_index_exists = os.path.exists(os.path.join(model_path_normalized, \"model_index.json\"))\n",
    "        if model_index_exists:\n",
    "            print(f\"✅ Cargando desde ruta local: {model_path_normalized}\")\n",
    "            pipe = DiffusionPipeline.from_pretrained(\n",
    "                model_path_normalized,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "            ).to(cfg.device)\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"❌ La ruta local {model_path_normalized} no contiene 'model_index.json'.\\n\"\n",
    "                \"Para descargar Karlo primero, ejecuta:\\n\"\n",
    "                \"  from diffusers import DiffusionPipeline\\n\"\n",
    "                \"  pipe = DiffusionPipeline.from_pretrained('kakaobrain/karlo-v1-alpha')\\n\"\n",
    "                f\"  pipe.save_pretrained(r'{model_path_normalized}')\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"📥 Descargando modelo desde HuggingFace Hub: {cfg.model_path}\")\n",
    "        pipe = DiffusionPipeline.from_pretrained(\n",
    "            cfg.model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=cfg.cache_dir,\n",
    "            trust_remote_code=True,\n",
    "        ).to(cfg.device)\n",
    "    pipe._offload_device = None\n",
    "    pipe.enable_model_cpu_offload = lambda *args, **kwargs: None\n",
    "    print(f\"✅ Pipeline cargada: {type(pipe).__name__}\")\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def get_decoder_unet(pipe: DiffusionPipeline):\n",
    "    if hasattr(pipe, \"decoder\"):\n",
    "        dec = pipe.decoder\n",
    "        return dec.unet if hasattr(dec, \"unet\") else dec\n",
    "    if hasattr(pipe, \"unet\"):\n",
    "        return pipe.unet\n",
    "    raise RuntimeError(\"No se encontró un UNet de decoder en la pipeline.\")\n",
    "\n",
    "\n",
    "def set_decoder_unet(pipe: DiffusionPipeline, new_unet: nn.Module) -> None:\n",
    "    if hasattr(pipe, \"decoder\"):\n",
    "        if hasattr(pipe.decoder, \"unet\"):\n",
    "            pipe.decoder.unet = new_unet\n",
    "        else:\n",
    "            pipe.decoder = new_unet\n",
    "    elif hasattr(pipe, \"unet\"):\n",
    "        pipe.unet = new_unet\n",
    "    else:\n",
    "        raise RuntimeError(\"No se pudo asignar el UNet de decoder en la pipeline.\")\n",
    "\n",
    "\n",
    "def add_lora_to_decoder(\n",
    "    pipe: DiffusionPipeline,\n",
    "    r: Optional[int] = None,\n",
    "    alpha: Optional[int] = None,\n",
    "    dropout: Optional[float] = None,\n",
    "    cfg: Optional[RLConfig] = None,\n",
    ") -> None:\n",
    "    if cfg is not None:\n",
    "        r = getattr(cfg, 'lora_rank', 8) if r is None else r\n",
    "        alpha = getattr(cfg, 'lora_alpha', 16) if alpha is None else alpha\n",
    "        dropout = getattr(cfg, 'lora_dropout', 0.1) if dropout is None else dropout\n",
    "    else:\n",
    "        r = 8 if r is None else r\n",
    "        alpha = 16 if alpha is None else alpha\n",
    "        dropout = 0.1 if dropout is None else dropout\n",
    "    print(f\"🔧 LoRA Configuration: r={r}, alpha={alpha}, dropout={dropout}\")\n",
    "\n",
    "    try:\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Instala 'peft': pip install peft\")\n",
    "\n",
    "    dec_unet = get_decoder_unet(pipe)\n",
    "\n",
    "    linear_names = [name for name, m in dec_unet.named_modules() if isinstance(m, nn.Linear)]\n",
    "    preferred = [\n",
    "        \"to_q\", \"to_k\", \"to_v\",\n",
    "        \"to_out.0\",\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",\n",
    "    ]\n",
    "    targets = sorted({name for name in linear_names if any(p in name for p in preferred)})\n",
    "    if not targets:\n",
    "        targets = linear_names\n",
    "        print(f\"[Aviso] No se hallaron patrones típicos; aplicaré LoRA a TODOS los Linear del decoder ({len(targets)}).\")\n",
    "    else:\n",
    "        print(f\"[LoRA] Módulos Linear objetivo detectados ({len(targets)}):\")\n",
    "        for t in targets[:20]:\n",
    "            print(\"   -\", t)\n",
    "        if len(targets) > 20:\n",
    "            print(f\"   ... y {len(targets)-20} más\")\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=targets,\n",
    "        lora_dropout=dropout,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    dec_unet_lora = get_peft_model(dec_unet, lora_cfg)\n",
    "    set_decoder_unet(pipe, dec_unet_lora)\n",
    "\n",
    "    for attr in [\"text_encoder\", \"prior\", \"super_res_first\", \"super_res_last\"]:\n",
    "        m = getattr(pipe, attr, None)\n",
    "        if m is None:\n",
    "            continue\n",
    "        if hasattr(m, \"unet\"):\n",
    "            for p in m.unet.parameters():\n",
    "                p.requires_grad = False\n",
    "        if hasattr(m, \"model\"):\n",
    "            for p in m.model.parameters():\n",
    "                p.requires_grad = False\n",
    "        if hasattr(m, \"parameters\"):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    for vae_attr in [\"vae\", \"decoder_vae\"]:\n",
    "        m = getattr(pipe, vae_attr, None)\n",
    "        if m is not None and hasattr(m, \"parameters\"):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    dec_unet_lora = get_decoder_unet(pipe)\n",
    "    trainable_params = 0\n",
    "    frozen_params = 0\n",
    "    for name, p in dec_unet_lora.named_parameters():\n",
    "        if \"lora_\" in name:\n",
    "            p.requires_grad = True\n",
    "            trainable_params += p.numel()\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "            frozen_params += p.numel()\n",
    "    print(f\"LoRA aplicada — parámetros entrenables en UNet (solo LoRA): {trainable_params:,}\")\n",
    "    print(f\"Parámetros congelados en UNet (base): {frozen_params:,}\")\n",
    "\n",
    "\n",
    "def load_clip_and_aesthetic_predictor(device: str):\n",
    "    path = hf_hub_download(\n",
    "        repo_id=\"trl-lib/ddpo-aesthetic-predictor\",\n",
    "        filename=\"aesthetic-model.pth\",\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    raw = torch.load(path, map_location=\"cpu\")\n",
    "    if isinstance(raw, dict) and \"state_dict\" in raw and isinstance(raw[\"state_dict\"], dict):\n",
    "        sd = raw[\"state_dict\"]\n",
    "    elif isinstance(raw, dict):\n",
    "        sd = raw\n",
    "    else:\n",
    "        raise ValueError(f\"Formato desconocido del checkpoint estético: tipo={type(raw)}\")\n",
    "    sd = {k.replace(\"module.\", \"\").replace(\"model.\", \"\"): v for k, v in sd.items()}\n",
    "    candidates = [\n",
    "        (\"linear.weight\", \"linear.bias\"),\n",
    "        (\"lin.weight\", \"lin.bias\"),\n",
    "        (\"aesthetic_head.weight\", \"aesthetic_head.bias\"),\n",
    "        (\"weight\", \"bias\"),\n",
    "    ]\n",
    "    w = b = None\n",
    "    for kw, kb in candidates:\n",
    "        if kw in sd and kb in sd:\n",
    "            w, b = sd[kw], sd[kb]\n",
    "            break\n",
    "    if w is None or b is None:\n",
    "        w_keys = [k for k in sd.keys() if k.endswith(\"weight\")]\n",
    "        b_keys = [k for k in sd.keys() if k.endswith(\"bias\")]\n",
    "        if w_keys and b_keys:\n",
    "            w, b = sd[w_keys[0]], sd[b_keys[0]]\n",
    "        else:\n",
    "            raise KeyError(\"No se encontraron claves de weight/bias en el predictor estético.\")\n",
    "    w = w.float(); b = b.float()\n",
    "    if w.dim() == 1:\n",
    "        w = w.view(1, -1)\n",
    "    elif w.dim() == 2 and w.shape[0] > 1:\n",
    "        w = w.mean(dim=0, keepdim=True)\n",
    "    elif w.dim() == 2 and w.shape[0] == 1:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Forma inesperada de weight: {tuple(w.shape)}\")\n",
    "    if b.numel() != 1:\n",
    "        b = b.flatten().mean().view(1)\n",
    "    else:\n",
    "        b = b.view(1)\n",
    "    clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "        \"ViT-L-14\", pretrained=\"openai\", cache_dir=None\n",
    "    )\n",
    "    clip_model = clip_model.to(device)\n",
    "    clip_model.eval()\n",
    "    tokenizer = open_clip.get_tokenizer(\"ViT-L-14\")\n",
    "    try:\n",
    "        emb_dim = clip_model.visual.output_dim\n",
    "    except Exception:\n",
    "        emb_dim = 768\n",
    "    if w.shape[1] != emb_dim:\n",
    "        print(f\"[Aviso] Dimensión de weight {w.shape[1]} != emb_dim {emb_dim}. Ajustando.\")\n",
    "        if w.shape[1] > emb_dim:\n",
    "            w = w[:, :emb_dim].contiguous()\n",
    "        else:\n",
    "            pad = torch.zeros(1, emb_dim - w.shape[1], device=w.device, dtype=w.dtype)\n",
    "            w = torch.cat([w, pad], dim=1)\n",
    "    w = w.to(device); b = b.to(device)\n",
    "    return clip_model, w, b, tokenizer\n",
    "\n",
    "\n",
    "def load_pickscore_model(device: str):\n",
    "    class _DummyPickScore:\n",
    "        def to(self, *_, **__): return self\n",
    "        def score(self, prompt: str, image): return 0.0\n",
    "    try:\n",
    "        from imscore import PickScore\n",
    "        model = PickScore()\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return model\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import pickscore\n",
    "        if hasattr(pickscore, \"load_model\"):\n",
    "            model = pickscore.load_model(device=device)\n",
    "            return model\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"[Aviso] PickScore no está instalado; se ignorará w_pickscore.\")\n",
    "    return _DummyPickScore()\n",
    "\n",
    "\n",
    "def load_hps_v2_model(device: str):\n",
    "    class _DummyHPS:\n",
    "        def to(self, *_, **__): return self\n",
    "        def score(self, prompt: str, image): return 0.0\n",
    "    try:\n",
    "        from imscore import HPS\n",
    "        model = HPS()\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return model\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from imscore import HPSv2\n",
    "        model = HPSv2()\n",
    "        try:\n",
    "            model = model.to(device)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return model\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"[Aviso] HPS v2 no está instalado; se ignorará w_hpsv2.\")\n",
    "    return _DummyHPS()\n",
    "\n",
    "\n",
    "def load_pickscore_model_real(device: str, model_path: str):\n",
    "    try:\n",
    "        from transformers import AutoProcessor, AutoModel\n",
    "        processor = AutoProcessor.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path).eval().to(device)\n",
    "        class _PickScoreLocal:\n",
    "            def __init__(self, processor, model):\n",
    "                self.processor = processor; self.model = model\n",
    "            def to(self, *_args, **_kwargs): return self\n",
    "            def score(self, prompt: str, image):\n",
    "                image_inputs = self.processor(images=[image], return_tensors=\"pt\").to(device)\n",
    "                text_inputs = self.processor(text=[prompt], return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    image_embs = self.model.get_image_features(**image_inputs)\n",
    "                    image_embs = image_embs / image_embs.norm(dim=-1, keepdim=True)\n",
    "                    text_embs = self.model.get_text_features(**text_inputs)\n",
    "                    text_embs = text_embs / text_embs.norm(dim=-1, keepdim=True)\n",
    "                    scores = self.model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n",
    "                return float(scores.item())\n",
    "        print(f\"✅ PickScore cargado correctamente desde '{model_path}'\")\n",
    "        return _PickScoreLocal(processor, model)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ No se pudo cargar PickScore desde la ruta '{model_path}': {e}\")\n",
    "        class _Dummy:\n",
    "            def to(self, *_args, **_kwargs): return self\n",
    "            def score(self, prompt, image): return 0.0\n",
    "        return _Dummy()\n",
    "\n",
    "\n",
    "def load_hpsv2_model_real(device: str, model_path: str):\n",
    "    try:\n",
    "        from transformers import AutoProcessor, AutoModel\n",
    "        processor = AutoProcessor.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path).eval().to(device)\n",
    "        class _HPSv2Local:\n",
    "            def __init__(self, processor, model):\n",
    "                self.processor = processor; self.model = model\n",
    "            def to(self, *_args, **_kwargs): return self\n",
    "            def score(self, prompt: str, image):\n",
    "                image_inputs = self.processor(images=[image], return_tensors=\"pt\").to(device)\n",
    "                text_inputs = self.processor(text=[prompt], return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    image_embs = self.model.get_image_features(**image_inputs)\n",
    "                    image_embs = image_embs / image_embs.norm(dim=-1, keepdim=True)\n",
    "                    text_embs = self.model.get_text_features(**text_inputs)\n",
    "                    text_embs = text_embs / text_embs.norm(dim=-1, keepdim=True)\n",
    "                    scores = self.model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n",
    "                return float(scores.item())\n",
    "        print(f\"✅ HPSv2 cargado correctamente desde '{model_path}'\")\n",
    "        return _HPSv2Local(processor, model)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ No se pudo cargar HPSv2 desde la ruta '{model_path}': {e}\")\n",
    "        class _Dummy:\n",
    "            def to(self, *_args, **_kwargs): return self\n",
    "            def score(self, prompt, image): return 0.0\n",
    "        return _Dummy()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Evaluators:\n",
    "    clip_model: Any\n",
    "    weight: torch.Tensor\n",
    "    bias: torch.Tensor\n",
    "    tokenizer: Any\n",
    "    pick_model: Any\n",
    "    hps_model: Any\n",
    "\n",
    "\n",
    "def load_all_evaluators(cfg: RLConfig) -> Evaluators:\n",
    "    print(\"📥 Cargando evaluadores (una sola vez)...\")\n",
    "    clip_model, weight, bias, tokenizer = load_clip_and_aesthetic_predictor(cfg.device)\n",
    "\n",
    "    if getattr(cfg, \"pickscore_model_path\", None):\n",
    "        pick_model = load_pickscore_model_real(cfg.device, cfg.pickscore_model_path)\n",
    "    else:\n",
    "        pick_model = load_pickscore_model(cfg.device)\n",
    "\n",
    "    if getattr(cfg, \"hpsv2_model_path\", None):\n",
    "        hps_model = load_hpsv2_model_real(cfg.device, cfg.hpsv2_model_path)\n",
    "    else:\n",
    "        hps_model = load_hps_v2_model(cfg.device)\n",
    "\n",
    "    print(\"✅ Evaluadores listos\")\n",
    "    return Evaluators(\n",
    "        clip_model=clip_model,\n",
    "        weight=weight,\n",
    "        bias=bias,\n",
    "        tokenizer=tokenizer,\n",
    "        pick_model=pick_model,\n",
    "        hps_model=hps_model,\n",
    "    )\n",
    "\n",
    "\n",
    "def _to_01(imgs: torch.Tensor) -> torch.Tensor:\n",
    "    if imgs.min() < 0:\n",
    "        imgs = (imgs.clamp(-1, 1) + 1) / 2\n",
    "    return imgs.clamp(0, 1)\n",
    "\n",
    "\n",
    "def aesthetic_score_from_tensor(\n",
    "    clip_model,\n",
    "    weight: torch.Tensor,\n",
    "    bias: torch.Tensor,\n",
    "    images_pt: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    imgs = _to_01(images_pt).float()\n",
    "    imgs = F.interpolate(imgs, size=224, mode=\"bicubic\", align_corners=False)\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=imgs.device)[None, :, None, None]\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=imgs.device)[None, :, None, None]\n",
    "    imgs = (imgs - mean) / std\n",
    "    emb = clip_model.encode_image(imgs)\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    scores = (emb @ weight.t()).squeeze(-1) + bias.squeeze()\n",
    "    return scores.float()\n",
    "\n",
    "\n",
    "def clip_text_image_alignment(\n",
    "    clip_model,\n",
    "    tokenizer,\n",
    "    images_pt: torch.Tensor,\n",
    "    prompts: List[str],\n",
    ") -> torch.Tensor:\n",
    "    imgs = _to_01(images_pt).float()\n",
    "    imgs = F.interpolate(imgs, size=224, mode=\"bicubic\", align_corners=False)\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=imgs.device)[None, :, None, None]\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=imgs.device)[None, :, None, None]\n",
    "    imgs = (imgs - mean) / std\n",
    "    img_feats = clip_model.encode_image(imgs)\n",
    "    img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n",
    "    with torch.no_grad():\n",
    "        toks = tokenizer(prompts)\n",
    "        if not torch.is_tensor(toks):\n",
    "            toks = torch.tensor(toks)\n",
    "        toks = toks.to(imgs.device)\n",
    "        if toks.dim() == 1:\n",
    "            toks = toks.unsqueeze(0)\n",
    "        txt_feats = clip_model.encode_text(toks)\n",
    "        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n",
    "    sims = (img_feats * txt_feats).sum(dim=-1)\n",
    "    return sims.float()\n",
    "\n",
    "\n",
    "def normalize_metric_per_batch(values: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    if values.numel() <= 1:\n",
    "        return torch.tanh(values)\n",
    "\n",
    "    mean = values.mean()\n",
    "    std = values.std().clamp(min=eps)\n",
    "    z = (values - mean) / std\n",
    "    z = torch.clamp(z, -5, 5)\n",
    "    normalized = torch.tanh(z)\n",
    "\n",
    "    if torch.isnan(normalized).any():\n",
    "        print(f\"⚠️ NaN en normalize_metric_per_batch: {values}\")\n",
    "        return torch.zeros_like(values)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def compute_reward_combined(\n",
    "    prompts: List[str],\n",
    "    images_m1_1: torch.Tensor,\n",
    "    clip_model,\n",
    "    tokenizer,\n",
    "    weight: torch.Tensor,\n",
    "    bias: torch.Tensor,\n",
    "    pick_model,\n",
    "    hps_model,\n",
    "    w_aesthetic: float,\n",
    "    w_clip_align: float,\n",
    "    w_pickscore: float,\n",
    "    w_hpsv2: float,\n",
    ") -> torch.Tensor:\n",
    "    aest = aesthetic_score_from_tensor(clip_model, weight, bias, images_m1_1)\n",
    "    align = clip_text_image_alignment(clip_model, tokenizer, images_m1_1, prompts)\n",
    "\n",
    "    pick_vals = []\n",
    "    hps_vals = []\n",
    "    imgs_01 = _to_01(images_m1_1)\n",
    "\n",
    "    for p, img in zip(prompts, imgs_01.cpu()):\n",
    "        pil = TVF.to_pil_image(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                pv = float(pick_model.score(p, pil))\n",
    "                pick_vals.append(pv)\n",
    "            except Exception:\n",
    "                pick_vals.append(0.5)\n",
    "\n",
    "            try:\n",
    "                hv = float(hps_model.score(p, pil))\n",
    "                hps_vals.append(hv)\n",
    "            except Exception:\n",
    "                hps_vals.append(0.5)\n",
    "\n",
    "    pick_t = torch.tensor(pick_vals, device=images_m1_1.device, dtype=aest.dtype)\n",
    "    hps_t = torch.tensor(hps_vals, device=images_m1_1.device, dtype=aest.dtype)\n",
    "\n",
    "    aest_norm = normalize_metric_per_batch(aest)\n",
    "    align_norm = normalize_metric_per_batch(align)\n",
    "    pick_norm = normalize_metric_per_batch(pick_t)\n",
    "    hps_norm = normalize_metric_per_batch(hps_t)\n",
    "\n",
    "    combined = (\n",
    "        w_aesthetic * aest_norm +\n",
    "        w_clip_align * align_norm +\n",
    "        w_pickscore * pick_norm +\n",
    "        w_hpsv2 * hps_norm\n",
    "    )\n",
    "\n",
    "    if torch.isnan(combined).any():\n",
    "        print(f\"⚠️ NaN en combined reward\")\n",
    "        print(f\"  aest_norm: {aest_norm}\")\n",
    "        print(f\"  align_norm: {align_norm}\")\n",
    "        print(f\"  pick_norm: {pick_norm}\")\n",
    "        print(f\"  hps_norm: {hps_norm}\")\n",
    "        return torch.zeros_like(combined)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def normalize_rewards(\n",
    "    rewards: torch.Tensor,\n",
    "    mode: str = \"batch\",\n",
    "    eps: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    if mode == \"none\":\n",
    "        return rewards\n",
    "    if mode == \"tanh\":\n",
    "        return torch.tanh(rewards)\n",
    "    mean = rewards.mean()\n",
    "    std = rewards.std().clamp_min(eps)\n",
    "    if std < 1e-5:\n",
    "        print(f\"[Aviso] Recompensas muy similares (std={std:.2e}). Retornando como están.\")\n",
    "        return rewards\n",
    "    normalized = (rewards - mean) / std\n",
    "    return normalized.clamp(-10, 10)\n",
    "\n",
    "\n",
    "def load_prompts(filepath: str) -> List[str]:\n",
    "    prompts: List[str] = []\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        print(f\"✅ Cargados {len(prompts)} prompts desde: {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: No se encontró el archivo {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al leer {filepath}: {type(e).__name__}: {e}\")\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def prepare_cv_datasets(cfg: RLConfig) -> Tuple[List[str], List[str]]:\n",
    "    train_prompts = load_prompts(cfg.train_prompts_file)\n",
    "    val_prompts = load_prompts(cfg.val_prompts_file)\n",
    "    all_prompts = train_prompts + val_prompts\n",
    "    if not all_prompts:\n",
    "        print(\"❌ Error: No se cargaron prompts de entrenamiento/validación.\")\n",
    "        return [], []\n",
    "    random.seed(cfg.seed)\n",
    "    random.shuffle(all_prompts)\n",
    "    test_size = max(1, int(len(all_prompts) * cfg.test_ratio)) if cfg.test_ratio > 0 else 0\n",
    "    test_prompts = all_prompts[:test_size]\n",
    "    cv_prompts = all_prompts[test_size:]\n",
    "    print(f\"🚧 División de datos: total={len(all_prompts)}, test={len(test_prompts)}, cv={len(cv_prompts)}\")\n",
    "    return test_prompts, cv_prompts\n",
    "\n",
    "\n",
    "def kfold_split(prompts: List[str], k: int, seed: int = 42) -> List[Tuple[List[str], List[str]]]:\n",
    "    n = len(prompts)\n",
    "    if k <= 1 or n < k:\n",
    "        return [(prompts, [])]\n",
    "    shuffled_prompts = prompts.copy()\n",
    "    random.Random(seed).shuffle(shuffled_prompts)\n",
    "    fold_size = n // k\n",
    "    splits: List[Tuple[List[str], List[str]]] = []\n",
    "    for i in range(k):\n",
    "        val_start = i * fold_size\n",
    "        val_end = (i + 1) * fold_size if i < k - 1 else n\n",
    "        val_prompts = shuffled_prompts[val_start:val_end]\n",
    "        train_prompts = shuffled_prompts[:val_start] + shuffled_prompts[val_end:]\n",
    "        splits.append((train_prompts, val_prompts))\n",
    "    return splits\n",
    "\n",
    "\n",
    "def call_pipe_compat(\n",
    "    pipe: DiffusionPipeline,\n",
    "    prompts: List[str],\n",
    "    cfg: RLConfig,\n",
    "    want_pt: bool = True,\n",
    "    override_prior_steps: Optional[int] = None,\n",
    "    override_decoder_steps: Optional[int] = None,\n",
    "    override_sr_steps: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    sig = inspect.signature(pipe.__call__).parameters\n",
    "    kwargs: Dict[str, Any] = {}\n",
    "    if \"prompt\" in sig:\n",
    "        kwargs[\"prompt\"] = prompts\n",
    "    elif \"text\" in sig:\n",
    "        kwargs[\"text\"] = prompts\n",
    "    prior_steps = max(2, int(override_prior_steps if override_prior_steps is not None else cfg.prior_steps))\n",
    "    decoder_steps = max(2, int(override_decoder_steps if override_decoder_steps is not None else cfg.decoder_steps))\n",
    "    sr_steps = max(2, int(override_sr_steps if override_sr_steps is not None else cfg.sr_steps))\n",
    "    if \"prior_num_inference_steps\" in sig:\n",
    "        kwargs[\"prior_num_inference_steps\"] = prior_steps\n",
    "    if \"decoder_num_inference_steps\" in sig:\n",
    "        kwargs[\"decoder_num_inference_steps\"] = decoder_steps\n",
    "    if \"super_res_num_inference_steps\" in sig:\n",
    "        kwargs[\"super_res_num_inference_steps\"] = sr_steps\n",
    "    if \"prior_guidance_scale\" in sig:\n",
    "        kwargs[\"prior_guidance_scale\"] = getattr(cfg, \"prior_guidance\", 4.0)\n",
    "    if \"decoder_guidance_scale\" in sig:\n",
    "        kwargs[\"decoder_guidance_scale\"] = getattr(cfg, \"decoder_guidance\", 7.5)\n",
    "    if want_pt and \"output_type\" in sig:\n",
    "        kwargs[\"output_type\"] = \"pt\"\n",
    "        kwargs[\"return_dict\"] = True\n",
    "    print(\n",
    "        f\"[UnCLIP] steps -> prior={prior_steps}, decoder={decoder_steps}, super_res={sr_steps}\"\n",
    "    )\n",
    "    out = pipe(**kwargs)\n",
    "    images = getattr(out, \"images\", out)\n",
    "    if isinstance(images, list):\n",
    "        images = torch.stack([TVF.to_tensor(img) for img in images], dim=0)\n",
    "    elif isinstance(images, torch.Tensor):\n",
    "        pass\n",
    "    else:\n",
    "        images = torch.from_numpy(images).permute(0, 3, 1, 2).contiguous()\n",
    "    return images.to(next(get_decoder_unet(pipe).parameters()).device)\n",
    "\n",
    "\n",
    "def evaluate_prompts(\n",
    "    pipe: DiffusionPipeline,\n",
    "    prompts: List[str],\n",
    "    cfg: RLConfig,\n",
    "    clip_model,\n",
    "    tokenizer,\n",
    "    weight: torch.Tensor,\n",
    "    bias: torch.Tensor,\n",
    "    pick_model,\n",
    "    hps_model,\n",
    ") -> Dict[str, float]:\n",
    "    if not prompts:\n",
    "        return {\n",
    "            \"aesthetic\": 0.0,\n",
    "            \"clip_align\": 0.0,\n",
    "            \"pickscore\": 0.0,\n",
    "            \"hpsv2\": 0.0,\n",
    "            \"aesthetic_norm\": 0.0,\n",
    "            \"clip_align_norm\": 0.0,\n",
    "            \"pickscore_norm\": 0.0,\n",
    "            \"hpsv2_norm\": 0.0,\n",
    "            \"weighted\": 0.0,\n",
    "        }\n",
    "    get_decoder_unet(pipe).eval()\n",
    "    total_aest = 0.0; total_align = 0.0; total_pick = 0.0; total_hps = 0.0\n",
    "    total_aest_norm = 0.0; total_align_norm = 0.0; total_pick_norm = 0.0; total_hps_norm = 0.0\n",
    "    count = 0\n",
    "    eval_batch_size = max(1, getattr(cfg, 'eval_batch_size', 2))\n",
    "    for i in range(0, len(prompts), eval_batch_size):\n",
    "        batch_prompts = prompts[i:i + eval_batch_size]\n",
    "        with torch.no_grad():\n",
    "            imgs01_batch = call_pipe_compat(\n",
    "                pipe,\n",
    "                batch_prompts,\n",
    "                cfg,\n",
    "                want_pt=True,\n",
    "                override_decoder_steps=(\n",
    "                    cfg.eval_decoder_steps if cfg.eval_decoder_steps is not None else cfg.decoder_steps\n",
    "                ),\n",
    "            )\n",
    "        imgs_m1_1_batch = imgs01_batch * 2 - 1\n",
    "        with torch.no_grad():\n",
    "            aest_batch = aesthetic_score_from_tensor(clip_model, weight, bias, imgs_m1_1_batch).detach().cpu()\n",
    "            align_batch = clip_text_image_alignment(clip_model, tokenizer, imgs_m1_1_batch, batch_prompts).detach().cpu()\n",
    "        total_aest += float(aest_batch.sum())\n",
    "        total_align += float(align_batch.sum())\n",
    "        total_aest_norm += float(torch.tanh(aest_batch).sum())\n",
    "        total_align_norm += float(torch.tanh(align_batch).sum())\n",
    "        pick_vals: List[float] = []\n",
    "        hps_vals: List[float] = []\n",
    "\n",
    "        for p, img_t in zip(batch_prompts, imgs01_batch.detach().cpu()):\n",
    "            pil = TVF.to_pil_image(img_t)\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    pv = float(pick_model.score(p, pil))\n",
    "                    pick_vals.append(pv)\n",
    "                except Exception:\n",
    "                    pick_vals.append(0.0)\n",
    "                try:\n",
    "                    hv = float(hps_model.score(p, pil))\n",
    "                    hps_vals.append(hv)\n",
    "                except Exception:\n",
    "                    hps_vals.append(0.0)\n",
    "\n",
    "        if pick_vals:\n",
    "            pick_tensor = torch.tensor(pick_vals, device=aest_batch.device, dtype=aest_batch.dtype)\n",
    "            pick_norm_tensor = normalize_metric_per_batch(pick_tensor)\n",
    "            total_pick += float(pick_tensor.sum())\n",
    "            total_pick_norm += float(pick_norm_tensor.sum())\n",
    "\n",
    "        if hps_vals:\n",
    "            hps_tensor = torch.tensor(hps_vals, device=aest_batch.device, dtype=aest_batch.dtype)\n",
    "            hps_norm_tensor = normalize_metric_per_batch(hps_tensor)\n",
    "            total_hps += float(hps_tensor.sum())\n",
    "            total_hps_norm += float(hps_norm_tensor.sum())\n",
    "\n",
    "        count += len(batch_prompts)\n",
    "        del imgs01_batch, imgs_m1_1_batch, aest_batch, align_batch\n",
    "        torch.cuda.empty_cache()\n",
    "    if count > 0:\n",
    "        aest_mean = total_aest / count\n",
    "        align_mean = total_align / count\n",
    "        pick_mean = total_pick / count\n",
    "        hps_mean = total_hps / count\n",
    "        aest_norm_mean = total_aest_norm / count\n",
    "        align_norm_mean = total_align_norm / count\n",
    "        pick_norm_mean = total_pick_norm / count\n",
    "        hps_norm_mean = total_hps_norm / count\n",
    "    else:\n",
    "        aest_mean = align_mean = pick_mean = hps_mean = 0.0\n",
    "        aest_norm_mean = align_norm_mean = pick_norm_mean = hps_norm_mean = 0.0\n",
    "    weighted = (\n",
    "        cfg.w_aesthetic * aest_norm_mean +\n",
    "        cfg.w_clip_align * align_norm_mean +\n",
    "        cfg.w_pickscore * pick_norm_mean +\n",
    "        cfg.w_hpsv2 * hps_norm_mean\n",
    "    )\n",
    "    return {\n",
    "        \"aesthetic\": aest_mean,\n",
    "        \"clip_align\": align_mean,\n",
    "        \"pickscore\": pick_mean,\n",
    "        \"hpsv2\": hps_mean,\n",
    "        \"aesthetic_norm\": aest_norm_mean,\n",
    "        \"clip_align_norm\": align_norm_mean,\n",
    "        \"pickscore_norm\": pick_norm_mean,\n",
    "        \"hpsv2_norm\": hps_norm_mean,\n",
    "        \"weighted\": weighted,\n",
    "    }\n",
    "\n",
    "\n",
    "def rl_training_loop_for_prompts(\n",
    "    cfg: RLConfig,\n",
    "    train_prompts: List[str],\n",
    "    val_prompts: List[str],\n",
    "    fold_index: int,\n",
    "    results_dir: str = \"cv_results\",\n",
    "    evaluators: Optional[Evaluators] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    device = cfg.device\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    fold_dir = os.path.join(results_dir, f\"fold_{fold_index:02d}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    pipe = load_unclip_pipeline(cfg)\n",
    "    add_lora_to_decoder(pipe, cfg=cfg)\n",
    "    unet = get_decoder_unet(pipe)\n",
    "    unet.train()\n",
    "    trainable_params = [p for p in unet.parameters() if p.requires_grad]\n",
    "    if not trainable_params:\n",
    "        raise RuntimeError(\"❌ No hay parámetros entrenables en el decoder\")\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=cfg.lr)\n",
    "\n",
    "    if evaluators is None:\n",
    "        evaluators = load_all_evaluators(cfg)\n",
    "\n",
    "    clip_model = evaluators.clip_model\n",
    "    weight = evaluators.weight\n",
    "    bias = evaluators.bias\n",
    "    tokenizer = evaluators.tokenizer\n",
    "    pick_model = evaluators.pick_model\n",
    "    hps_model = evaluators.hps_model\n",
    "\n",
    "    print(f\"🚀 Iniciando entrenamiento del fold {fold_index+1}/{cfg.k_folds}...\")\n",
    "\n",
    "    train_metrics_history: List[Dict[str, float]] = []\n",
    "    val_metrics_history: List[Dict[str, float]] = []\n",
    "    best_weighted_score = -float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for step in range(1, cfg.steps + 1):\n",
    "        batch = random.sample(train_prompts, k=min(cfg.batch_size, len(train_prompts)))\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        micro_losses: List[float] = []\n",
    "        raw_rewards_all: List[float] = []\n",
    "        for _ in range(cfg.grad_accum_steps):\n",
    "            dec_steps_train = (\n",
    "                cfg.train_decoder_steps\n",
    "                if cfg.train_decoder_steps is not None\n",
    "                else cfg.decoder_steps\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "            imgs01 = call_pipe_compat(\n",
    "                pipe,\n",
    "                batch,\n",
    "                cfg,\n",
    "                want_pt=True,\n",
    "                override_decoder_steps=dec_steps_train,\n",
    "            )\n",
    "            imgs_m1_1 = imgs01 * 2 - 1\n",
    "            rewards = compute_reward_combined(\n",
    "                prompts=batch,\n",
    "                images_m1_1=imgs_m1_1,\n",
    "                clip_model=clip_model,\n",
    "                tokenizer=tokenizer,\n",
    "                weight=weight,\n",
    "                bias=bias,\n",
    "                pick_model=pick_model,\n",
    "                hps_model=hps_model,\n",
    "                w_aesthetic=cfg.w_aesthetic,\n",
    "                w_clip_align=cfg.w_clip_align,\n",
    "                w_pickscore=cfg.w_pickscore,\n",
    "                w_hpsv2=cfg.w_hpsv2,\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                raw_mean = float(rewards.detach().cpu().mean().item())\n",
    "            raw_rewards_all.append(raw_mean)\n",
    "            rewards_norm = normalize_rewards(rewards, cfg.reward_norm)\n",
    "            loss = -rewards_norm.mean() / cfg.grad_accum_steps\n",
    "            try:\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print(f\"[Aviso] Backprop falló: {type(e).__name__}: {e}\")\n",
    "                micro_losses.append(float(loss.detach().cpu().item()))\n",
    "                continue\n",
    "            micro_losses.append(float(loss.detach().cpu().item()))\n",
    "            del imgs01, imgs_m1_1, rewards, rewards_norm, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        try:\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, cfg.max_grad_norm)\n",
    "            optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"[Aviso] Optimizer step falló: {type(e).__name__}: {e}\")\n",
    "        try:\n",
    "            train_metrics_step = evaluate_prompts(\n",
    "                pipe,\n",
    "                batch,\n",
    "                cfg,\n",
    "                clip_model,\n",
    "                tokenizer,\n",
    "                weight,\n",
    "                bias,\n",
    "                pick_model,\n",
    "                hps_model,\n",
    "            )\n",
    "        except Exception:\n",
    "            train_metrics_step = {\n",
    "                \"weighted\": 0.0,\n",
    "                \"aesthetic\": 0.0,\n",
    "                \"clip_align\": 0.0,\n",
    "                \"pickscore\": 0.0,\n",
    "                \"hpsv2\": 0.0,\n",
    "                \"aesthetic_norm\": 0.0,\n",
    "                \"clip_align_norm\": 0.0,\n",
    "                \"pickscore_norm\": 0.0,\n",
    "                \"hpsv2_norm\": 0.0,\n",
    "            }\n",
    "        train_metrics_history.append(train_metrics_step)\n",
    "        try:\n",
    "            val_metrics_step = evaluate_prompts(\n",
    "                pipe,\n",
    "                val_prompts,\n",
    "                cfg,\n",
    "                clip_model,\n",
    "                tokenizer,\n",
    "                weight,\n",
    "                bias,\n",
    "                pick_model,\n",
    "                hps_model,\n",
    "            )\n",
    "        except Exception:\n",
    "            val_metrics_step = {\n",
    "                \"weighted\": 0.0,\n",
    "                \"aesthetic\": 0.0,\n",
    "                \"clip_align\": 0.0,\n",
    "                \"pickscore\": 0.0,\n",
    "                \"hpsv2\": 0.0,\n",
    "                \"aesthetic_norm\": 0.0,\n",
    "                \"clip_align_norm\": 0.0,\n",
    "                \"pickscore_norm\": 0.0,\n",
    "                \"hpsv2_norm\": 0.0,\n",
    "            }\n",
    "        val_metrics_history.append(val_metrics_step)\n",
    "        current_weighted = val_metrics_step['weighted']\n",
    "        if current_weighted > best_weighted_score + cfg.min_delta:\n",
    "            best_weighted_score = current_weighted\n",
    "            patience_counter = 0\n",
    "            best_model_state = get_decoder_unet(pipe).state_dict().copy()\n",
    "            print(f\"✅ Mejora detectada! Nuevo mejor score: {best_weighted_score:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⏳ Sin mejora ({patience_counter}/{cfg.early_stopping_patience}) - Mejor score: {best_weighted_score:.4f}\")\n",
    "        if patience_counter >= cfg.early_stopping_patience:\n",
    "            print(f\"🛑 Early stopping activado después de {step} pasos sin mejora\")\n",
    "            if best_model_state is not None:\n",
    "                get_decoder_unet(pipe).load_state_dict(best_model_state)\n",
    "            break\n",
    "        avg_loss = sum(micro_losses) / len(micro_losses) if micro_losses else float('nan')\n",
    "        avg_raw_reward = sum(raw_rewards_all) / len(raw_rewards_all) if raw_rewards_all else 0.0\n",
    "        print(\n",
    "            f\"[Fold {fold_index+1}, Paso {step:03d}] \"\n",
    "            f\"Loss: {avg_loss:.4f}, Reward: {avg_raw_reward:.4f} | \"\n",
    "            f\"Train -> aest: {train_metrics_step['aesthetic']:.4f}, align: {train_metrics_step['clip_align']:.4f}, \"\n",
    "            f\"pick: {train_metrics_step['pickscore']:.4f}, hps: {train_metrics_step['hpsv2']:.4f}, \"\n",
    "            f\"weighted: {train_metrics_step['weighted']:.4f} | \"\n",
    "            f\"Val -> aest: {val_metrics_step['aesthetic']:.4f}, align: {val_metrics_step['clip_align']:.4f}, \"\n",
    "            f\"pick: {val_metrics_step['pickscore']:.4f}, hps: {val_metrics_step['hpsv2']:.4f}, \"\n",
    "            f\"weighted: {val_metrics_step['weighted']:.4f}\"\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    if best_model_state is not None:\n",
    "        get_decoder_unet(pipe).load_state_dict(best_model_state)\n",
    "    lora_path = os.path.join(fold_dir, f\"lora_weights_fold_{fold_index:02d}.pth\")\n",
    "    os.makedirs(os.path.dirname(lora_path), exist_ok=True)\n",
    "\n",
    "    unet = get_decoder_unet(pipe)\n",
    "    lora_weights = {k: v for k, v in unet.state_dict().items() if \"lora_\" in k}\n",
    "    payload = {\n",
    "        \"lora_state_dict\": lora_weights,\n",
    "        \"lora_rank\": cfg.lora_rank,\n",
    "        \"lora_alpha\": cfg.lora_alpha,\n",
    "        \"lora_dropout\": cfg.lora_dropout,\n",
    "    }\n",
    "    torch.save(payload, lora_path)\n",
    "    print(f\"✅ Guardados {len(lora_weights)} pesos LoRA en {lora_path}\")\n",
    "    print(f\"✅ Fold {fold_index+1}: entrenamiento completado. Pesos guardados en {lora_path}\")\n",
    "    val_metrics = evaluate_prompts(\n",
    "        pipe,\n",
    "        val_prompts,\n",
    "        cfg,\n",
    "        clip_model,\n",
    "        tokenizer,\n",
    "        weight,\n",
    "        bias,\n",
    "        pick_model,\n",
    "        hps_model,\n",
    "    )\n",
    "    return {\n",
    "        \"fold\": fold_index,\n",
    "        \"metrics\": val_metrics,\n",
    "        \"weights_path\": lora_path,\n",
    "        \"train_metrics_history\": train_metrics_history,\n",
    "        \"val_metrics_history\": val_metrics_history,\n",
    "        \"best_weighted_score\": best_weighted_score,\n",
    "        \"early_stopped\": patience_counter >= cfg.early_stopping_patience,\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_validation_training(\n",
    "    cfg: RLConfig,\n",
    "    k_folds: Optional[int] = None,\n",
    "    test_ratio: Optional[float] = None,\n",
    "    results_dir: str = \"cv_results\",\n",
    "    return_results: bool = True,\n",
    "    evaluators: Optional[Evaluators] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    if k_folds is None:\n",
    "        k_folds = cfg.k_folds\n",
    "    if test_ratio is None:\n",
    "        test_ratio = cfg.test_ratio\n",
    "    cfg.k_folds = k_folds\n",
    "    cfg.test_ratio = test_ratio\n",
    "\n",
    "    random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(cfg.seed)\n",
    "\n",
    "    print(f\"\\n🎲 Realizando validación cruzada con {k_folds} folds y test_ratio={test_ratio:.2f}\\n\")\n",
    "    print(f\"⚡ Early stopping configurado: paciencia={cfg.early_stopping_patience}, min_delta={cfg.min_delta}\\n\")\n",
    "\n",
    "    train_prompts_file_prompts = load_prompts(cfg.train_prompts_file)\n",
    "    val_prompts_file_prompts = load_prompts(cfg.val_prompts_file)\n",
    "    all_prompts = train_prompts_file_prompts + val_prompts_file_prompts\n",
    "    if not all_prompts:\n",
    "        print(\"❌ Error: No se cargaron prompts de entrenamiento/validación.\")\n",
    "        return {}\n",
    "\n",
    "    random.shuffle(all_prompts)\n",
    "    test_prompts: List[str] = []\n",
    "    if test_ratio and test_ratio > 0:\n",
    "        test_size = max(1, int(len(all_prompts) * test_ratio))\n",
    "        test_prompts = all_prompts[:test_size]\n",
    "        cv_prompts = all_prompts[test_size:]\n",
    "    else:\n",
    "        cv_prompts = all_prompts\n",
    "\n",
    "    if not cv_prompts:\n",
    "        print(\"❌ Abortando cross-validation: no hay datos suficientes para dividir en folds.\")\n",
    "        return {}\n",
    "\n",
    "    splits = kfold_split(cv_prompts, k_folds, cfg.seed)\n",
    "\n",
    "    if evaluators is None:\n",
    "        evaluators = load_all_evaluators(cfg)\n",
    "\n",
    "    fold_results: List[Dict[str, Any]] = []\n",
    "    for i, (train_prompts_fold, val_prompts_fold) in enumerate(splits):\n",
    "        print(f\"\\n=== Fold {i+1}/{k_folds}: entrenamiento con {len(train_prompts_fold)} prompts y validación con {len(val_prompts_fold)} prompts ===\")\n",
    "        fold_result = rl_training_loop_for_prompts(\n",
    "            cfg,\n",
    "            train_prompts_fold,\n",
    "            val_prompts_fold,\n",
    "            i,\n",
    "            results_dir=results_dir,\n",
    "            evaluators=evaluators,\n",
    "        )\n",
    "        fold_results.append(fold_result)\n",
    "\n",
    "    best_fold = max(fold_results, key=lambda fr: fr[\"metrics\"][\"weighted\"])\n",
    "    print(f\"\\n🏆 Mejor fold: {best_fold['fold']+1} con puntuación ponderada {best_fold['metrics']['weighted']:.4f}\")\n",
    "    if best_fold.get('early_stopped', False):\n",
    "        print(f\"   ⚡ Este fold usó early stopping (se detuvo anticipadamente)\")\n",
    "\n",
    "    pipe_final = load_pipeline_with_lora(cfg, best_fold[\"weights_path\"])\n",
    "\n",
    "    clip_model = evaluators.clip_model\n",
    "    weight = evaluators.weight\n",
    "    bias = evaluators.bias\n",
    "    tokenizer = evaluators.tokenizer\n",
    "    pick_model = evaluators.pick_model\n",
    "    hps_model = evaluators.hps_model\n",
    "\n",
    "    best_train_prompts, best_val_prompts = splits[best_fold[\"fold\"]]\n",
    "\n",
    "    metrics_train = evaluate_prompts(\n",
    "        pipe_final,\n",
    "        best_train_prompts,\n",
    "        cfg,\n",
    "        clip_model,\n",
    "        tokenizer,\n",
    "        weight,\n",
    "        bias,\n",
    "        pick_model,\n",
    "        hps_model,\n",
    "    ) if best_train_prompts else {}\n",
    "    metrics_val = evaluate_prompts(\n",
    "        pipe_final,\n",
    "        best_val_prompts,\n",
    "        cfg,\n",
    "        clip_model,\n",
    "        tokenizer,\n",
    "        weight,\n",
    "        bias,\n",
    "        pick_model,\n",
    "        hps_model,\n",
    "    )\n",
    "    metrics_test = None\n",
    "    if test_prompts:\n",
    "        metrics_test = evaluate_prompts(\n",
    "            pipe_final,\n",
    "            test_prompts,\n",
    "            cfg,\n",
    "            clip_model,\n",
    "            tokenizer,\n",
    "            weight,\n",
    "            bias,\n",
    "            pick_model,\n",
    "            hps_model,\n",
    "        )\n",
    "\n",
    "    summary_csv = os.path.join(results_dir, \"cv_summary.csv\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    fieldnames = [\n",
    "        \"fold\", \"weighted\", \"aesthetic\", \"clip_align\", \"pickscore\", \"hpsv2\",\n",
    "        \"early_stopped\", \"best_weighted\"\n",
    "    ]\n",
    "    with open(summary_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for fr in fold_results:\n",
    "            row = {\n",
    "                \"fold\": fr[\"fold\"] + 1,\n",
    "                \"weighted\": fr[\"metrics\"][\"weighted\"],\n",
    "                \"aesthetic\": fr[\"metrics\"][\"aesthetic\"],\n",
    "                \"clip_align\": fr[\"metrics\"][\"clip_align\"],\n",
    "                \"pickscore\": fr[\"metrics\"][\"pickscore\"],\n",
    "                \"hpsv2\": fr[\"metrics\"][\"hpsv2\"],\n",
    "                \"early_stopped\": fr.get(\"early_stopped\", False),\n",
    "                \"best_weighted\": fr.get(\"best_weighted_score\", 0.0),\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "        if metrics_train:\n",
    "            writer.writerow({\n",
    "                \"fold\": \"final_train\",\n",
    "                \"weighted\": metrics_train.get(\"weighted\", 0.0),\n",
    "                \"aesthetic\": metrics_train.get(\"aesthetic\", 0.0),\n",
    "                \"clip_align\": metrics_train.get(\"clip_align\", 0.0),\n",
    "                \"pickscore\": metrics_train.get(\"pickscore\", 0.0),\n",
    "                \"hpsv2\": metrics_train.get(\"hpsv2\", 0.0),\n",
    "                \"early_stopped\": \"\",\n",
    "                \"best_weighted\": \"\",\n",
    "            })\n",
    "        writer.writerow({\n",
    "            \"fold\": \"final_val\",\n",
    "            \"weighted\": metrics_val.get(\"weighted\", 0.0),\n",
    "            \"aesthetic\": metrics_val.get(\"aesthetic\", 0.0),\n",
    "            \"clip_align\": metrics_val.get(\"clip_align\", 0.0),\n",
    "            \"pickscore\": metrics_val.get(\"pickscore\", 0.0),\n",
    "            \"hpsv2\": metrics_val.get(\"hpsv2\", 0.0),\n",
    "            \"early_stopped\": \"\",\n",
    "            \"best_weighted\": \"\",\n",
    "        })\n",
    "        if metrics_test is not None:\n",
    "            writer.writerow({\n",
    "                \"fold\": \"final_test\",\n",
    "                \"weighted\": metrics_test.get(\"weighted\", 0.0),\n",
    "                \"aesthetic\": metrics_test.get(\"aesthetic\", 0.0),\n",
    "                \"clip_align\": metrics_test.get(\"clip_align\", 0.0),\n",
    "                \"pickscore\": metrics_test.get(\"pickscore\", 0.0),\n",
    "                \"hpsv2\": metrics_test.get(\"hpsv2\", 0.0),\n",
    "                \"early_stopped\": \"\",\n",
    "                \"best_weighted\": \"\",\n",
    "            })\n",
    "    print(f\"📊 Resumen de validación cruzada guardado en {summary_csv}\")\n",
    "    print(\"\\n📈 Métricas finales del modelo seleccionado:\")\n",
    "    if metrics_train:\n",
    "        print(f\"  Entrenamiento: {metrics_train}\")\n",
    "    print(f\"  Validación:   {metrics_val}\")\n",
    "    if metrics_test is not None:\n",
    "        print(f\"  Test:         {metrics_test}\")\n",
    "\n",
    "    try:\n",
    "        plot_training_validation_metrics(\n",
    "            best_fold.get(\"train_metrics_history\", []),\n",
    "            best_fold.get(\"val_metrics_history\", []),\n",
    "            results_dir,\n",
    "            prefix=f\"fold_{best_fold['fold']+1}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] No se pudieron generar las gráficas de entrenamiento/validación: {e}\")\n",
    "    if metrics_test is not None:\n",
    "        try:\n",
    "            plot_test_metrics(\n",
    "                metrics_test,\n",
    "                results_dir,\n",
    "                prefix=f\"fold_{best_fold['fold']+1}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Aviso] No se pudieron generar las gráficas de test: {e}\")\n",
    "\n",
    "    if return_results:\n",
    "        return {\n",
    "            \"best_fold_index\": best_fold[\"fold\"],\n",
    "            \"best_fold_metrics\": best_fold[\"metrics\"],\n",
    "            \"best_weights_path\": best_fold[\"weights_path\"],\n",
    "            \"metrics_train\": metrics_train,\n",
    "            \"metrics_val\": metrics_val,\n",
    "            \"metrics_test\": metrics_test,\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "\n",
    "def sample_hyperparameters(cfg: RLConfig) -> RLConfig:\n",
    "    new_cfg = deepcopy(cfg)\n",
    "    new_cfg.lora_rank = random.choice(cfg.lora_rank_options)\n",
    "    new_cfg.lora_alpha = random.choice(cfg.lora_alpha_options)\n",
    "    new_cfg.lora_dropout = random.choice(cfg.lora_dropout_options)\n",
    "    new_cfg.lr = random.choice(cfg.lr_options)\n",
    "    def pick_weight(options: Tuple[float, ...]) -> float:\n",
    "        return random.choice(options) if options else random.random()\n",
    "    w_a = pick_weight(cfg.w_aesthetic_options)\n",
    "    w_c = pick_weight(cfg.w_clip_align_options)\n",
    "    w_p = pick_weight(cfg.w_pickscore_options)\n",
    "    w_h = pick_weight(cfg.w_hpsv2_options)\n",
    "    total_w = w_a + w_c + w_p + w_h\n",
    "    if total_w == 0:\n",
    "        w_a = w_c = w_p = w_h = 0.25\n",
    "        total_w = 1.0\n",
    "    new_cfg.w_aesthetic = w_a / total_w\n",
    "    new_cfg.w_clip_align = w_c / total_w\n",
    "    new_cfg.w_pickscore = w_p / total_w\n",
    "    new_cfg.w_hpsv2 = w_h / total_w\n",
    "    return new_cfg\n",
    "\n",
    "\n",
    "def random_search_cross_validation(cfg: RLConfig) -> Dict[str, Any]:\n",
    "    assert cfg.param_search_n > 0, \"La búsqueda aleatoria requiere param_search_n > 0\"\n",
    "    best_result: Optional[Dict[str, Any]] = None\n",
    "    best_score: float = -float('inf')\n",
    "    for i in range(cfg.param_search_n):\n",
    "        print(f\"\\n⚙️  Búsqueda aleatoria {i+1}/{cfg.param_search_n}\")\n",
    "        sampled_cfg = sample_hyperparameters(cfg)\n",
    "        print(\n",
    "            f\"   -> LoRA: rank={sampled_cfg.lora_rank}, alpha={sampled_cfg.lora_alpha}, dropout={sampled_cfg.lora_dropout}\\n\"\n",
    "            f\"   -> LR={sampled_cfg.lr:.1e}, Pesos recompensa=[{sampled_cfg.w_aesthetic:.2f}, {sampled_cfg.w_clip_align:.2f}, {sampled_cfg.w_pickscore:.2f}, {sampled_cfg.w_hpsv2:.2f}]\"\n",
    "        )\n",
    "        try:\n",
    "            result = cross_validation_training(sampled_cfg, return_results=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[Aviso] La combinación {i+1} falló: {e}\")\n",
    "            continue\n",
    "        score = result.get(\"metrics_val\", {}).get(\"weighted\", 0.0) if result else 0.0\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_result = {**result, \"config\": deepcopy(sampled_cfg)}\n",
    "            print(f\"✅ Nueva mejor combinación con score {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"❌ Combinación descartada: score {score:.4f} (mejor hasta ahora {best_score:.4f})\")\n",
    "    print(\"\\n🔍 Búsqueda aleatoria completada.\")\n",
    "    if best_result:\n",
    "        cfg_best = best_result[\"config\"]\n",
    "        print(\n",
    "            \"\\n🏁 Mejor configuración encontrada:\\n\"\n",
    "            f\"   -> LoRA rank={cfg_best.lora_rank}, alpha={cfg_best.lora_alpha}, dropout={cfg_best.lora_dropout}\\n\"\n",
    "            f\"   -> LR={cfg_best.lr:.1e}, Pesos recompensa=[{cfg_best.w_aesthetic:.2f}, {cfg_best.w_clip_align:.2f}, {cfg_best.w_pickscore:.2f}, {cfg_best.w_hpsv2:.2f}]\\n\"\n",
    "            f\"   -> Métrica de validación combinada: {best_score:.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"❌ No se encontró ninguna configuración válida durante la búsqueda aleatoria.\")\n",
    "    return best_result or {}\n",
    "\n",
    "\n",
    "def load_pipeline_with_lora(cfg: RLConfig, weights_path: str) -> DiffusionPipeline:\n",
    "    pipe = load_unclip_pipeline(cfg)\n",
    "    add_lora_to_decoder(pipe, cfg=cfg)\n",
    "    if weights_path and os.path.exists(weights_path):\n",
    "        try:\n",
    "            payload = torch.load(weights_path, map_location=cfg.device)\n",
    "            if isinstance(payload, dict) and \"lora_state_dict\" in payload:\n",
    "                state_dict = payload[\"lora_state_dict\"]\n",
    "                print(\n",
    "                    f\"ℹ️ Cargando LoRA con rank={payload.get('lora_rank')}, \"\n",
    "                    f\"alpha={payload.get('lora_alpha')}, dropout={payload.get('lora_dropout')}\"\n",
    "                )\n",
    "            else:\n",
    "                state_dict = payload\n",
    "\n",
    "            get_decoder_unet(pipe).load_state_dict(state_dict, strict=False)\n",
    "            print(f\"✅ Pesos LoRA cargados desde {weights_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al cargar pesos LoRA desde {weights_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"[Aviso] No se encontró el archivo de pesos {weights_path}; se utilizará el modelo base\")\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def plot_training_validation_metrics(\n",
    "    train_history: List[Dict[str, float]],\n",
    "    val_history: List[Dict[str, float]],\n",
    "    output_dir: str,\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    if not train_history or not val_history:\n",
    "        return\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    steps = list(range(1, len(train_history) + 1))\n",
    "    metrics = [\"aesthetic\", \"clip_align\", \"pickscore\", \"hpsv2\", \"weighted\"]\n",
    "    for metric in metrics:\n",
    "        train_vals = [h.get(metric, 0.0) for h in train_history]\n",
    "        val_vals = [h.get(metric, 0.0) for h in val_history]\n",
    "        plt.figure()\n",
    "        plt.plot(steps, train_vals, label=f\"Train {metric}\")\n",
    "        plt.plot(steps, val_vals, label=f\"Val {metric}\")\n",
    "        plt.xlabel(\"Paso\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{prefix} {metric} vs Paso\")\n",
    "        plt.legend()\n",
    "        filename = f\"{prefix}_{metric}_curve.png\"\n",
    "        plt.savefig(os.path.join(output_dir, filename))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_test_metrics(metrics: Dict[str, float], output_dir: str, prefix: str) -> None:\n",
    "    if not metrics:\n",
    "        return\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metric_names = [\"aesthetic\", \"clip_align\", \"pickscore\", \"hpsv2\", \"weighted\"]\n",
    "    values = [metrics.get(name, 0.0) for name in metric_names]\n",
    "    plt.figure()\n",
    "    plt.bar(metric_names, values)\n",
    "    plt.xlabel(\"Métrica\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.title(f\"{prefix} Test Metrics\")\n",
    "    filename = f\"{prefix}_test_metrics.png\"\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_test_images(\n",
    "    pipe: DiffusionPipeline,\n",
    "    cfg: RLConfig,\n",
    "    test_prompts: List[str],\n",
    "    output_dir: str = \"test_samples\",\n",
    ") -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    batch_size = getattr(cfg, 'eval_batch_size', 2)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎨 Guardando imágenes de test en: \" + output_dir)\n",
    "    print(\"📊 Total de prompts: \" + str(len(test_prompts)))\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    for i in range(0, len(test_prompts), batch_size):\n",
    "        batch_prompts = test_prompts[i:i + batch_size]\n",
    "        with torch.no_grad():\n",
    "            images = call_pipe_compat(\n",
    "                pipe,\n",
    "                batch_prompts,\n",
    "                cfg,\n",
    "                want_pt=True,\n",
    "                override_decoder_steps=40,\n",
    "                override_sr_steps=10,\n",
    "            )\n",
    "        for j, img in enumerate(images):\n",
    "            prompt_idx = i + j\n",
    "            if prompt_idx < len(test_prompts):\n",
    "                prompt = test_prompts[prompt_idx]\n",
    "                img_filename = f\"test_img_{prompt_idx:04d}.png\"\n",
    "                img_save_path = os.path.join(output_dir, img_filename)\n",
    "                save_image(img.cpu(), img_save_path)\n",
    "                prompt_filename = f\"test_img_{prompt_idx:04d}_prompt.txt\"\n",
    "                prompt_save_path = os.path.join(output_dir, prompt_filename)\n",
    "                with open(prompt_save_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(prompt)\n",
    "                print(f\"✅ [{prompt_idx+1}/{len(test_prompts)}] Imagen guardada: {img_save_path}\")\n",
    "                print(f\"   📝 Prompt guardado: {prompt_save_path}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ Todas las imágenes de test guardadas exitosamente!\")\n",
    "    print(\"   📁 Directorio: \" + os.path.abspath(output_dir))\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "def save_test_images_with_trained_lora(\n",
    "    cfg: RLConfig,\n",
    "    weights_path: str,\n",
    "    test_prompts: List[str],\n",
    "    output_dir: str = \"test_samples\",\n",
    ") -> None:\n",
    "    pipe = load_pipeline_with_lora(cfg, weights_path)\n",
    "    save_test_images(pipe, cfg, test_prompts, output_dir)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"⚙️  CONFIGURACIÓN DE LoRA - ENTRENAMIENTO RLHF KARLO\")\n",
    "    print(\"=\" * 80)\n",
    "    cfg = RLConfig()\n",
    "    print(\"\\n📝 Configurando parámetros de LoRA:\")\n",
    "    print(f\"   ✅ lora_rank = {cfg.lora_rank}\")\n",
    "    print(f\"   ✅ lora_alpha = {cfg.lora_alpha}\")\n",
    "    print(f\"   ✅ lora_dropout = {cfg.lora_dropout}\")\n",
    "    cfg.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"   ✅ device = {cfg.device}\")\n",
    "    cfg.train_decoder_steps = cfg.decoder_steps\n",
    "    cfg.eval_decoder_steps = cfg.decoder_steps\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CONFIGURACIÓN COMPLETA DE RUTAS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"📁 Modelo: {cfg.model_path}\")\n",
    "    print(f\"📁 Cache: {cfg.cache_dir}\")\n",
    "    print(f\"📁 Prompts entrenamiento: {cfg.train_prompts_file}\")\n",
    "    print(f\"📁 Prompts validación: {cfg.val_prompts_file}\")\n",
    "    print(f\"📁 Evaluación: {cfg.eval_dir}\")\n",
    "    print(f\"📁 PickScore: {cfg.pickscore_model_path}\")\n",
    "    print(f\"📁 HPS v2: {cfg.hpsv2_model_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if cfg.param_search_n and cfg.param_search_n > 0:\n",
    "        print(\"\\n🔁 Ejecutando búsqueda aleatoria de hiperparámetros...\")\n",
    "        search_result = random_search_cross_validation(cfg)\n",
    "        if search_result and 'best_weights_path' in search_result:\n",
    "            test_prompts, _ = prepare_cv_datasets(cfg)\n",
    "            if test_prompts:\n",
    "                print(\"\\n🎨 Generando imágenes de test con la mejor configuración encontrada...\")\n",
    "                save_test_images_with_trained_lora(cfg, search_result['best_weights_path'], test_prompts)\n",
    "    else:\n",
    "        print(\"\\n🚀 INICIANDO VALIDACIÓN CRUZADA POR DEFECTO\")\n",
    "        cv_result = cross_validation_training(cfg, return_results=True)\n",
    "        test_prompts, _ = prepare_cv_datasets(cfg)\n",
    "        if cv_result and test_prompts:\n",
    "            print(\"\\n🎨 Generando imágenes de test con el mejor fold...\")\n",
    "            save_test_images_with_trained_lora(cfg, cv_result['best_weights_path'], test_prompts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
